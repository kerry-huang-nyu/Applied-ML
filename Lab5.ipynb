{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60b9d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits #import the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534267fa",
   "metadata": {},
   "source": [
    "### Load + Visualize MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1434e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  9.  9.  4.  0.  0.  0.]\n",
      " [ 0.  0. 15. 15. 14. 12.  0.  0.]\n",
      " [ 0.  3. 10.  1.  0. 12.  5.  0.]\n",
      " [ 0.  5.  8.  0.  0.  8.  6.  0.]\n",
      " [ 0.  8.  8.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0. 10.  6.  0.]\n",
      " [ 0.  4. 13.  4.  6. 13.  0.  0.]\n",
      " [ 0.  0.  6. 16. 14.  3.  0.  0.]]\n",
      "[ 0.  0.  9.  9.  4.  0.  0.  0.  0.  0. 15. 15. 14. 12.  0.  0.  0.  3.\n",
      " 10.  1.  0. 12.  5.  0.  0.  5.  8.  0.  0.  8.  6.  0.  0.  8.  8.  0.\n",
      "  0.  8.  8.  0.  0.  5.  8.  0.  0. 10.  6.  0.  0.  4. 13.  4.  6. 13.\n",
      "  0.  0.  0.  0.  6. 16. 14.  3.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "example = 1 \n",
    "#means that we have 64 pixels in an 8 by 8 manner \n",
    "#smaller than the mnist with 28 x 28 pixels \n",
    "\n",
    "\n",
    "# Plot some sample digits\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for i in range(10):\n",
    "#     plt.subplot(2, 5, i + 1)\n",
    "#     plt.imshow(digits.images[i], cmap='gray')\n",
    "#     plt.title(f\"Label: {digits.target[i]}\")\n",
    "#     plt.axis('off')\n",
    "image = X_train[example].reshape(8,8)\n",
    "#see they are different values since reshape makes a new copy\n",
    "print(image)\n",
    "print(X_train[example])\n",
    "#image[0][0] = 2\n",
    "#tried to modify the image and saw that most likely the image brightness \n",
    "#is represented in comparison. If a rogue pixel is 100, then all else with pale in comparison\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76363d",
   "metadata": {},
   "source": [
    "### Scaling dataset features using Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b13f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 is now normalized. Mean is 0 and standard deviation of brightness is 1.\n",
      "[ 0.         -0.34169755  0.78471641 -0.65860017 -1.84460249 -1.03722581\n",
      " -0.40623424 -0.13101874 -0.06103492 -0.61725402  0.8589375   0.75430467\n",
      "  0.76199765  0.60700536 -0.52465505 -0.13336005 -0.04991522  0.12851911\n",
      "  0.03778099 -1.04767677 -1.16010978  0.65736168  0.96852751 -0.11409248\n",
      " -0.03733267  0.86365151 -0.15567903 -1.49945054 -1.60835913  0.07651178\n",
      "  0.98733438 -0.04573894  0.          1.67384566  0.07355384 -1.44495478\n",
      " -1.74526375 -0.10676512  1.45911053  0.         -0.06519029  1.16537582\n",
      "  0.17429724 -1.13725613 -1.23504336  0.30352474  0.58424608 -0.09403434\n",
      " -0.03963009  1.84547567  0.96531158 -1.07225551 -0.66033986  0.68917912\n",
      " -0.76379714 -0.21608405 -0.02638899 -0.30677646  0.08503346  0.89164719\n",
      "  0.43903596 -0.64451929 -0.50623083 -0.19710003]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"Sample 1 is now normalized. Mean is 0 and standard deviation of brightness is 1.\")\n",
    "print(X_train[example])\n",
    "\n",
    "image = X_train[example].reshape(8,8)\n",
    "#image[0][0] = 2\n",
    "#tried to modify the image and saw that most likely the image brightness \n",
    "#is represented in comparison. If a rogue pixel is 100, then all else with pale in comparison\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18084d4",
   "metadata": {},
   "source": [
    "### One Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "912e2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 0 ... 2 7 1]\n",
      "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "#the idea here is just to use a bitmap to encode each category \n",
    "#for example, 100 means the 3rd category. \n",
    "#then also in the end the neural network should output the probabilities of each encoding \n",
    "#in our example. the index of the predictions is the number it is \n",
    "\n",
    "def hot(y):\n",
    "    print(y)\n",
    "    hot = [[int(bit) for bit in bin(2**val)[2:].zfill(10)] for val in y]\n",
    "    return hot \n",
    "\n",
    "print(hot(y_train)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84ed39",
   "metadata": {},
   "source": [
    "### Different Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0c44aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#functions \n",
    "\n",
    "def sigmoid(z):\n",
    "    bottom = 1 + np.exp(-z)\n",
    "    return 1/bottom \n",
    "\n",
    "def relu(z):\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    return z \n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "#derivatives \n",
    "    \n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1- sigmoid(z))\n",
    "    \n",
    "def d_relu(z):\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    return 1\n",
    "    \n",
    "def d_tanh(z):\n",
    "    return 1 - (tanh(z) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e0f2f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(X, y, activation, d_activation, train = True, weights = None, lr = 0.0001, epoch = 100):\n",
    "    #train determines if you do backwards propagation and improve as a result \n",
    "    \n",
    "    activation = np.vectorize(activation)\n",
    "    d_activation = np.vectorize(activation)\n",
    "    \n",
    "    y = np.array(hot(y))\n",
    "    print(\"y is actually 1430 features 10 predictions \", y.shape)\n",
    "    \n",
    "    #forward pass start \n",
    "    #each row in X represents an image. \n",
    "    #assumed that X would in # of samples x 64 pixels \n",
    "    \n",
    "    w1 = np.random.rand(64, 30)\n",
    "    b1 = np.random.rand(1, 30) #can numpy broadcast this into the len of X? \n",
    "    \n",
    "    w2 = np.random.rand(30, 10)\n",
    "    b2 = np.random.rand(1, 10) #numpy broadcast this into len of X \n",
    "    \n",
    "    if not train and weights: #when not training, and have weights pass the weights and start testing \n",
    "        w1, b1, w2, b2 = weights\n",
    "    \n",
    "    #the theory is that we apply 30 weights to each neuron \n",
    "    #z11 = x1 * w1,1 + x2 * w1,2 + x3 * w1,3 .... x64 * w1,64\n",
    "    #z12 = x1 * w2,1 + x2 * w2,2 + x3 * w2,3 .... \n",
    "    \n",
    "    #then each x from X will go through the same weights \n",
    "    \n",
    "    for _ in range(epoch):\n",
    "        print(\"round: \", _)\n",
    "    \n",
    "        z1 = X @ w1 + b1 \n",
    "        #print(\"z1\", z1.shape)\n",
    "        z1_active = activation(z1) #element wise \n",
    "        #print(\"z1_active\", z1_active.shape)\n",
    "\n",
    "        z2 = z1_active @ w2 + b2 \n",
    "        #print(\"z2\", z2.shape)\n",
    "        z2_active = activation(z2)\n",
    "        #print(\"z2_active\", z2_active.shape)\n",
    "\n",
    "        #forward pass end. find the error \n",
    "        error = np.square(z2_active - y)\n",
    "        #print(\"error\", error.shape)\n",
    "\n",
    "        pred = np.array([np.argmax(row) for row in z2_active])\n",
    "        #print(\"pred\", pred.shape)\n",
    "\n",
    "        #only if training do we go through with back propagation passes \n",
    "        if train: \n",
    "            #back propagation pass \n",
    "            dE_dyhat = 2 * (z2_active - y)\n",
    "            dyhat_dz2 = d_activation(z2_active)\n",
    "\n",
    "            dz2_dw2 = z1_active.T\n",
    "            dE_db = (dE_dyhat * dyhat_dz2) #since it is just 1 \n",
    "            dE_dw = dz2_dw2 @ (dE_db)\n",
    "\n",
    "            #update 1st weight + bias \n",
    "            dz2_dA1 = w2.T\n",
    "            \n",
    "            w2 = w2 - lr * dE_dw\n",
    "            b2 = b2 - lr * dE_db \n",
    "\n",
    "\n",
    "            #back propagation pass \n",
    "            dA1_dz1 = d_activation(z1_active)\n",
    "            dz1_dw1 = X.T\n",
    "\n",
    "            #print(\"my shape isss \", (dE_dyhat * dyhat_dz2).shape)\n",
    "            #print(\"other shape isss\", dz2_dA1.T.shape)\n",
    "            dE_db = ((dE_dyhat * dyhat_dz2) @ (dz2_dA1)) * dA1_dz1\n",
    "            \n",
    "            #print(\"last shape is \", dE_db.shape)\n",
    "            #print(\"X shape is\", X.shape)\n",
    "            dE_dw = dz1_dw1 @ (dE_db)\n",
    "\n",
    "\n",
    "            #update 2nd weight + bias \n",
    "            w1 = w1 - lr * dE_dw\n",
    "            b1 = b1 - lr * dE_db\n",
    "\n",
    "    return (pred, (w1, b1, w2, b2)) #activation and the error in the activations \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38d223",
   "metadata": {},
   "source": [
    "### Accuracy Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c138967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 0 ... 2 7 1]\n",
      "y is actually 1430 features 10 predictions  (1437, 10)\n",
      "round:  0\n",
      "[6 0 0 ... 2 7 1]\n",
      "y is actually 1430 features 10 predictions  (1437, 10)\n",
      "round:  0\n",
      "round:  1\n",
      "round:  2\n",
      "round:  3\n",
      "round:  4\n",
      "round:  5\n",
      "round:  6\n",
      "round:  7\n",
      "round:  8\n",
      "round:  9\n",
      "round:  10\n",
      "round:  11\n",
      "round:  12\n",
      "round:  13\n",
      "round:  14\n",
      "round:  15\n",
      "round:  16\n",
      "round:  17\n",
      "round:  18\n",
      "round:  19\n"
     ]
    }
   ],
   "source": [
    "results_1 = neural(X_train, y_train, tanh, d_tanh, train = False, weights = None, lr = 0.001, epoch = 1)\n",
    "\n",
    "results_100 = neural(X_train, y_train, tanh, d_tanh, train = True, weights = None, lr = 0.01, epoch = 20)\n",
    "#pass in the weights to test \n",
    "#neural(X_test, y_test, sigmoid, d_sigmoid, train = False, weights = None, lr = 0.0001, epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0a2a6279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no training:  0.14683368128044538\n",
      "training:  0.10368823938761308\n"
     ]
    }
   ],
   "source": [
    "num_correct_1 = np.sum(y_train == results_1[0])\n",
    "num_correct_100 = np.sum(y_train == results_100[0])\n",
    "print(\"no training: \", num_correct_1/len(y_train))\n",
    "print(\"training: \", num_correct_100/len(y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
